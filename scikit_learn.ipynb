{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## `scikit-learn` is a widely used machine learning library which has a large collection of supervised and unsupervised machine learning algorithms implemented.\n",
        "\n",
        "## Below are some examples of useful functionalities provided in the library.\n",
        ""
      ],
      "metadata": {
        "id": "9_5pNfinn5e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "iasmoRjWoaV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Toy Datasets\n",
        "\n",
        "#### `scikit-learn` has a few small standard datasets that don't need to be downloaded from the web. These are very useful and testing and understanding algorithms before running on a big dataset.\n",
        "#### This includes dataset for both classification and regression tasks. Let's import one such dataset called diabetes dataset and print some basic statistics of the dataset."
      ],
      "metadata": {
        "id": "dz0QMTCPqqy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes = datasets.load_diabetes()\n",
        "print('Shape of the dataset :', diabetes.data.shape)\n",
        "print('Shape of the labels:', diabetes.target.shape)\n",
        "print('features in the dataset:', diabetes.feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXPUyGuFrdaL",
        "outputId": "de48191b-0b08-480d-8468-593d1534800f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the dataset : (442, 10)\n",
            "Shape of the labels: (442,)\n",
            "features in the dataset: ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train/Test Split\n",
        "#### One of the most important step for training any machine learning models is to create a good train/test split of your dataset. `scikit-learnt` provides a very handy and quick utility to create test/train split. This utility also performs input validation and optional subsampling.\n",
        "\n",
        "#### The following code performs a 75% train and 25% test split of the diabetes dataset loaded in the previous step. We are also passing `random state` to make sure that we can reproduce the result as this utility by default randomly shuffles the dataset."
      ],
      "metadata": {
        "id": "GLEpPSOasp6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and test sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.25, random_state=42)\n",
        "\n",
        "print(\"Train features shape: \", X_train.shape)\n",
        "print(\"Train labels: \", Y_train.shape)\n",
        "print(\"Test features shape: \", X_test.shape)\n",
        "print(\"Test labels: \", Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7i5eo0Ruewu",
        "outputId": "93da86de-0ae2-4ddf-cfd7-169ab0d55a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train features shape:  (331, 10)\n",
            "Train labels:  (331,)\n",
            "Test features shape:  (111, 10)\n",
            "Test labels:  (111,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression\n",
        "#### linear regression is one of the most basic yet useful machine learning model. It's widely used for simple forecasting tasks. In the next cell, we will fit a linear regression model using `LinearRegression` class."
      ],
      "metadata": {
        "id": "yeQgMYnv6G1l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VO32nRNI7TiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import LinearRegression class\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# instantiate LinearRegression class\n",
        "linear_regression = LinearRegression()\n",
        "\n",
        "# Fit linear regression model on the training dataset created above\n",
        "linear_regression.fit(X_train, Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgmZQTntrcVG",
        "outputId": "2a340360-1bb0-49da-9073-619c56d3c3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After training the model, we will predict labels in the test dataset"
      ],
      "metadata": {
        "id": "yjCKyExf7Wm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will perform prediction on the test dataset\n",
        "Y_predictions = linear_regression.predict(X_test)\n",
        "\n",
        "print(\"Test predictions shape: \", Y_predictions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG9UXwE8rcYk",
        "outputId": "8fba613e-842c-4f13-d6ea-3cfc4dfbd8bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test predictions shape:  (111,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Error/Loss metrics\n",
        "#### Calculating the accuracy of a model is another important functionality to know how well has your model learnt and able to generalize that learning. These metrics are called `prediction error` or `prediction loss`.\n",
        "\n",
        "#### `scikit-learn` provides a lot of built-in methods to calculate prediction errors. Two most popular error metrics are `mse` or `mean squared error` and `mae` or `mean absolute error`. In next cell, we will compute both these metrics on predictions generated above."
      ],
      "metadata": {
        "id": "HvV0KqxhobyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "# Calculate mean squared error which is the mean of the squared difference between predictions and actuals\n",
        "mse = mean_squared_error(Y_test, Y_predictions)\n",
        "# Calculate mean absoulte error which is the mean of the absolute difference between predictions and actuals\n",
        "mae = mean_absolute_error(Y_test, Y_predictions)\n",
        "\n",
        "print(\"Mean squared error: %4f\" % mse)\n",
        "print(\"Mean absolute error: %4f\" % mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kro-3N9T99g3",
        "outputId": "a27ccc54-7519-4c50-dde2-b25407bf6b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 2848.295308\n",
            "Mean absolute error: 41.548363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM (Support Vector Machine) Model**\n",
        "\n",
        "### Used for classification and regression to find a hyperplane in a N-dimensional space that distinctly classifies data points. This is useful for classification tasks where classes can't be separated linearly in lower dimensions. So, we project them to a higher dimension space where classes are linearly separable."
      ],
      "metadata": {
        "id": "bjamydUYE_j2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `scikit-learn` also has built-in class to implement support vector machine. In the next cells, we will load breast cancer toy dataset and fit a support vector machine model on the training data."
      ],
      "metadata": {
        "id": "N5_e4ZGCN3dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's load the dataset and split into train and test datasets. We will use stratified split in this case as sometimes data can have large imbalances. For example, there could be many more negative examples compared to positive examples which could lead to model only learning pronounced classes and still able to achieve high accuracy."
      ],
      "metadata": {
        "id": "UO34ijy3OSzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load breast cancer toy dataset\n",
        "b_cancer = datasets.load_breast_cancer()\n",
        "X = b_cancer.data\n",
        "Y = b_cancer.target\n",
        "\n",
        "# Create training and test split with stratified sampling\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42, stratify=Y)"
      ],
      "metadata": {
        "id": "3kCbdBegOhfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "#### Most modern datasets can't be used directly to train a model. There could be missing values, really large max feature values or categorical features. This could make it hard to train a model on raw data due to uncertainity on how to fix missing values or expensive hardware requirements. To address these, one of the key steps in a machine learning workflow is to preprocess the data.\n",
        "\n",
        "#### `scikit-learn preprocessing` provides some standard algorithms to preprocess the data such as StandardScalar. After scaling raw data, normalized data will have zero mean and unit variance."
      ],
      "metadata": {
        "id": "f339I75pPgyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "# Fit StandardScalar which will compute mean and scale of all the features\n",
        "sc.fit(X_train)\n",
        "\n",
        "# let's print mean and scale of raw data\n",
        "print(\"Raw data mean: \", sc.mean_)\n",
        "print(\"Raw data scale: \", sc.scale_)\n",
        "\n",
        "print(\"=============================================================================================\")\n",
        "print(\"=============================================================================================\")\n",
        "\n",
        "X_train_scaled = sc.transform(X_train)\n",
        "X_test_scaled = sc.transform(X_test)\n",
        "print(\"Scaled train data mean: {} std: {} \".format(X_train_scaled.mean(axis=0), X_train_scaled.std(axis=0)))\n",
        "print(\"Scaled test data mean: {} std: {} \".format(X_test_scaled.mean(axis=0), X_test_scaled.std(axis=0)))"
      ],
      "metadata": {
        "id": "IA8YyDvM-0Pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa799ca2-492e-4bf7-ea72-42b7b744fab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data mean:  [1.40752019e+01 1.92950469e+01 9.15929108e+01 6.49627230e+02\n",
            " 9.60178404e-02 1.03459577e-01 8.89306613e-02 4.82109366e-02\n",
            " 1.80202582e-01 6.27792958e-02 3.98923239e-01 1.21372300e+00\n",
            " 2.82511291e+00 3.91714390e+01 7.07850939e-03 2.55824695e-02\n",
            " 3.24967761e-02 1.16545986e-02 2.07900657e-02 3.81400728e-03\n",
            " 1.62025423e+01 2.57082629e+01 1.06803732e+02 8.72524413e+02\n",
            " 1.32365258e-01 2.56040775e-01 2.77706495e-01 1.14155988e-01\n",
            " 2.91209624e-01 8.40794601e-02]\n",
            "Raw data scale:  [3.50479948e+00 4.44258216e+00 2.41565929e+01 3.45533249e+02\n",
            " 1.33298715e-02 5.26037833e-02 8.08417973e-02 3.84561113e-02\n",
            " 2.76122781e-02 7.22760813e-03 2.63030497e-01 5.69417242e-01\n",
            " 1.94955244e+00 4.12775756e+01 3.08496788e-03 1.84850051e-02\n",
            " 3.27951744e-02 6.33488329e-03 8.85972255e-03 2.83941769e-03\n",
            " 4.80276118e+00 6.30299305e+00 3.34093443e+01 5.58478747e+02\n",
            " 2.26226334e-02 1.62163195e-01 2.19105068e-01 6.71725169e-02\n",
            " 6.52951977e-02 1.87265961e-02]\n",
            "=============================================================================================\n",
            "=============================================================================================\n",
            "Scaled train data mean: [-4.68430719e-15  3.92226679e-16 -2.25693225e-15  1.54962115e-15\n",
            " -4.46955983e-16 -8.11166470e-16 -4.72496325e-16  1.49411000e-15\n",
            " -3.40338086e-15  9.71575454e-15 -4.23818192e-16 -1.24092181e-15\n",
            "  2.16816002e-16  2.34293544e-16  5.92118946e-16  4.11381935e-16\n",
            "  7.94356756e-16  7.62040405e-16 -2.39766475e-16  6.55969801e-16\n",
            " -2.34554160e-17  3.82557835e-15  9.72617917e-16 -1.27884140e-15\n",
            "  4.28374081e-15 -3.37497375e-16  6.93498467e-16  3.12738880e-16\n",
            " -2.22800391e-15 -2.47852078e-15] std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1.] \n",
            "Scaled test data mean: [ 0.05913795 -0.00483514  0.06195409  0.06059361  0.10221998  0.06667079\n",
            " -0.0064648   0.07327781  0.13823554  0.01008246  0.09452962  0.02187502\n",
            "  0.0835711   0.11236383 -0.04840717 -0.02245787 -0.07316891  0.08890213\n",
            " -0.11127549 -0.02677058  0.05521657 -0.01959508  0.05448546  0.05741632\n",
            "  0.00058672 -0.04357135 -0.10020894  0.02667005 -0.06910802 -0.02839639] std: [1.01692647 0.86250438 1.01837941 1.06691784 1.1978409  1.01059412\n",
            " 0.93993973 1.03001752 0.96017754 0.90068525 1.19556699 0.86522815\n",
            " 1.1348386  1.3549296  0.88417997 0.86521813 0.61816456 0.88507757\n",
            " 0.6852188  0.68552673 1.02044145 0.89310258 1.01826114 1.07087954\n",
            " 1.03296626 0.87100732 0.78394979 0.90773349 0.76377168 0.84568099] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit the model on scaled training dataset and performance of the model\n",
        "#### `scikit-learn.metrics` also provides different ways to calculate accuracy of the model such as calculating precision which is ratio of (True Positives) / (True Positives + False Positives), recall which is ratio of (True Positive) / (True Positives + False Negatives) and accuracy."
      ],
      "metadata": {
        "id": "8Mi_4y_VSAG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate SVM classifier and fit on training dataset\n",
        "svc = SVC(C=1.0, random_state=1, kernel='linear')\n",
        "svc.fit(X_train_scaled, Y_train)\n",
        "\n",
        "# let's check the performance of our model\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "\n",
        "Y_predict = svc.predict(X_test_scaled)\n",
        "\n",
        "print(\"Accuracy score %.3f\" % accuracy_score(Y_test, Y_predict))\n",
        "print(\"Recall: \", recall_score(Y_test, Y_predict))\n",
        "print(\"Precision: \" , precision_score(Y_test, Y_predict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6HLtKajRof5",
        "outputId": "6b0f66aa-5492-4c7a-9611-93d7dac06ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score 0.986\n",
            "Recall:  0.9888888888888889\n",
            "Precision:  0.9888888888888889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification report\n",
        "#### Another useful functionality is to create a classfication report to evaluate the model performance. This function calculates precision, recall, f1-score, accuracy and number of support examples for each class."
      ],
      "metadata": {
        "id": "0ZlFvO9BUZf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(Y_test, Y_predict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29BqNo0ETlEC",
        "outputId": "711bf78f-23d7-4b02-d01a-55574e457a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98        53\n",
            "           1       0.99      0.99      0.99        90\n",
            "\n",
            "    accuracy                           0.99       143\n",
            "   macro avg       0.99      0.99      0.99       143\n",
            "weighted avg       0.99      0.99      0.99       143\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix\n",
        "#### Yet another useful functionality is to calculate confusion matrix which helps in visualizing model performance. This is specially useful in multiclass classification problems to see which classes have the most mislabels etc."
      ],
      "metadata": {
        "id": "3Y2Q2WjtU9_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(Y_test, Y_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsc6oCuuUwDd",
        "outputId": "7c52a179-d460-4a23-c6de-d466f09270e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[52,  1],\n",
              "       [ 1, 89]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YQgPVWXzVcoK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}